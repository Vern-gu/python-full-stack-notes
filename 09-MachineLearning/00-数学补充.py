# 欧式距离：即勾股定理的扩展补充，计算n维空间中两点的距离，归一化之后的欧式距离称之为标准欧式距离
#################################################################################################
# 向量的点乘：a·b = |a|·|b|·cosθ
#################################################################################################
# 矩阵：
#   只有同型矩阵才可以加减
#   一个矩阵的列数等于另一个矩阵的行数时，这两个矩阵才可以相乘（行*列）（矩阵乘法不满足交换律）
#   把矩阵A的行和列互相交换所产生的矩阵称为A的转置矩阵，这一过程称为矩阵的转置
#   实部相等，虚部互为相反数的复数称之为共轭复数
#   矩阵平移：x' = x+t, y' = y+t  (x',y' 表示移动后像素点的坐标, t为移动的距离)
#   矩阵旋转：x = rcos(θ), y = csin(θ); (θ为像素点初始的坐标角度，φ为旋转的角度)
#             x' = rcos(φ+θ) = xcos(θ)-ysin(θ), y' = rcos(φ+θ) = ycos(θ)+xsin(θ)
#   矩阵缩放：x' = xt, y' = yt  (t为缩放的倍率)
#################################################################################################
# sigmoid函数(神经网络阈值/激活函数)：sigmoid = 1/(1+(e^-z))   (e=2.718, z = m*x+b, 函数取值范围为(0,1) )

# softmax运算（归一化指数函数）：根据n个数来求出每个数相应的概率。P(A)=e^A / (e^A + e^B + e^C ..)

# 欧拉公式：e^(ix) = cosx + isinx 当x=Π时公式为 e^(iΠ)+1 = 0
#   其表示的是一个随着时间变化，在复平面上做圆周运动的点。随着时间的改变，在时间轴上就成了一条螺旋线
#################################################################################################
# 概率：
#   联合概率：包含多个条件，且所有条件同时成立的概率，记作 P(A,B)
#   条件概率：一个事件(A)在另外一个事件(B)已经发生条件下的概率，记作 P(A|B)
#   相互独立：如果 P(A,B)=P(A)P(B) ，则称事件A与事件B相互独立

# 贝叶斯公式：P(C|W) = P(W|C)P(C) / P(W)        W为给定文档的特征值，C为文档类别
#   若计算时出现某个概率为0，则需要加上拉普拉斯平滑系数：P(F1|C)=(Ni+α)/(N+mα)
#   N为文档中满足条件的总词数，Ni为满足条件的F1出现的次数，α一般都为1，m为文档中出现的特征词的种类数
#################################################################################################
# 信息论：
#   基础：获得的信息可以消除随机不确定性
#   信息熵：用于衡量所获得信息可以消除多少的不确定性（把信息中排除了冗余后的平均信息量称为“信息熵”）单位为比特bit
#        (公式： -∑P(xi)logbP(xi) 底数b一般取2)
#   信息增益：知道某一信息后，不确定性的减少程度（g(D,A) = H(D) - H(D|A)  信息增益=信息熵-条件熵）
#################################################################################################
# 集成学习方法：通过建立几个模型组合地来解决单一的预测问题。
#   其原理为生成多个分类器/模型，各自独立的学习和做出预测，这些预测最后结合成组合预测，因此会优于任何一个单分类做出的预测
#   随机森林即为集成学习方法中的一种

